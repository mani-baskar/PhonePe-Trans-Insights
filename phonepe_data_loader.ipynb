{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "666f6d0f",
   "metadata": {},
   "source": [
    "# PhonePe Pulse Data Loader\n",
    "\n",
    "This notebook demonstrates how to load PhonePe Pulse JSON data into a MariaDB database.\n",
    "It covers cloning the PhonePe Pulse GitHub repository, reading various JSON file formats,\n",
    "converting them into tabular form using Pandas, and inserting the data into appropriate\n",
    "MariaDB tables with duplicate checking. The notebook is self-contained and can be run\n",
    "from top to bottom. Feel free to modify the file paths or uncomment the dataset\n",
    "sections you wish to process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60049a0b",
   "metadata": {},
   "source": [
    "### DataBase Connectivity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "290e7af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sqlalchemy import create_engine, MetaData\n",
    "from sqlalchemy.orm import sessionmaker\n",
    "\n",
    "db_user = 'root'\n",
    "db_password = 'RajaSri%4007'    # URL-encoded @\n",
    "db_host = '192.168.1.5'\n",
    "db_port = '3306'\n",
    "db_name = 'PhonePeV2'\n",
    "\n",
    "class DBConnect:\n",
    "    def __init__(self):\n",
    "        self.engine = create_engine(\n",
    "            f\"mysql+pymysql://{db_user}:{db_password}@{db_host}:{db_port}/{db_name}\",\n",
    "            echo=False,\n",
    "        )\n",
    "        # reflect once at start\n",
    "        self.metadata = MetaData()\n",
    "        self.metadata.reflect(bind=self.engine)\n",
    "        self.Session = sessionmaker(bind=self.engine)\n",
    "\n",
    "    def refresh_metadata(self):\n",
    "        \"\"\"If you created new tables in another session, call this before insert.\"\"\"\n",
    "        self.metadata.clear()\n",
    "        self.metadata.reflect(bind=self.engine)\n",
    "\n",
    "    def insert(self, table_name: str, data: dict):\n",
    "        # ensure we see newly created tables\n",
    "        if table_name not in self.metadata.tables:\n",
    "            self.refresh_metadata()\n",
    "        if table_name not in self.metadata.tables:\n",
    "            raise RuntimeError(f\"Table {table_name} not found in database.\")\n",
    "        table = self.metadata.tables[table_name]\n",
    "        with self.Session() as session:\n",
    "            # IMPORTANT: expand dict\n",
    "            session.execute(table.insert().values(**data))\n",
    "            session.commit()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f0aedf4",
   "metadata": {},
   "source": [
    "### *** DON'T FORGET TO IMPORT DBTable.SQL INTO YOUR CURRENT DATABASE *** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1eae8594",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "from git import Repo\n",
    "from sqlalchemy import select, and_\n",
    "\n",
    "# Repository configuration\n",
    "REPO_URL = 'https://github.com/PhonePe/pulse.git'\n",
    "REPO_PATH = 'GitFolder'\n",
    "\n",
    "# Define paths for aggregated, map, and top datasets\n",
    "# Aggregated datasets\n",
    "agg_user_PATH  = os.path.join(REPO_PATH, 'data', 'aggregated', 'user',        'country', 'india')\n",
    "agg_trans_PATH = os.path.join(REPO_PATH, 'data', 'aggregated', 'transaction', 'country', 'india')\n",
    "agg_ins_PATH   = os.path.join(REPO_PATH, 'data', 'aggregated', 'insurance',   'country', 'india')\n",
    "\n",
    "# Map datasets\n",
    "map_user_PATH     = os.path.join(REPO_PATH, 'data', 'map', 'user',        'hover',  'country', 'india')\n",
    "map_trans_PATH    = os.path.join(REPO_PATH, 'data', 'map', 'transaction', 'hover',  'country', 'india')\n",
    "map_insHover_PATH = os.path.join(REPO_PATH, 'data', 'map', 'insurance', 'hover',  'country', 'india')\n",
    "map_insCntry_PATH = os.path.join(REPO_PATH, 'data', 'map', 'insurance',           'country', 'india')\n",
    "\n",
    "# Top datasets\n",
    "top_user_PATH  = os.path.join(REPO_PATH, 'data', 'top', 'user',        'country', 'india')\n",
    "top_trans_PATH = os.path.join(REPO_PATH, 'data', 'top', 'transaction', 'country', 'india')\n",
    "top_ins_PATH   = os.path.join(REPO_PATH, 'data', 'top', 'insurance',   'country', 'india')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "969822c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone or update the PhonePe Pulse repository\n",
    "if not os.path.exists(REPO_PATH):\n",
    "    print(\"Cloning GitHub repository...\")\n",
    "    Repo.clone_from(REPO_URL, REPO_PATH)\n",
    "else:\n",
    "    try:\n",
    "        repo = Repo(REPO_PATH)\n",
    "        origin = repo.remotes.origin\n",
    "        origin.fetch()\n",
    "        repo.git.checkout('master')\n",
    "        origin.pull('master')\n",
    "        print(\"Repository updated.\")\n",
    "    except Exception as e:\n",
    "        print(\"Repository update skipped due to error:\", e)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5686d08",
   "metadata": {},
   "source": [
    "### Helper Functions\n",
    "\n",
    "The following functions assist with loading JSON files, normalizing values, checking for existing\n",
    "rows in the database to avoid duplicates, and inserting rows into the database."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b1cfb02",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load a JSON file from disk\n",
    "def load_json(file_path: str) -> dict:\n",
    "    with open(file_path, 'r', encoding='utf-8') as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# Convert Python values to DB-safe scalars (convert dict/list to JSON string)\n",
    "def to_db_scalar(value):\n",
    "    if value is None or (isinstance(value, float) and pd.isna(value)):\n",
    "        return None\n",
    "    if isinstance(value, (dict, list)):\n",
    "        return json.dumps(value, ensure_ascii=False)\n",
    "    return value\n",
    "\n",
    "# Normalize year and quarter fields\n",
    "def normalize_year_quarter(row: dict) -> dict:\n",
    "    if 'year' in row and row['year'] is not None:\n",
    "        try:\n",
    "            row['year'] = int(row['year'])\n",
    "        except Exception:\n",
    "            pass\n",
    "    if 'quarter' in row and row['quarter'] is not None:\n",
    "        q_str = str(row['quarter']).upper().lstrip('Q')\n",
    "        try:\n",
    "            row['quarter'] = int(q_str)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return row\n",
    "\n",
    "# Natural keys used to identify duplicates for each table\n",
    "KEY_COLUMNS = {\n",
    "    'agg_user':     ['state', 'year', 'quarter'],\n",
    "    'agg_trans':    ['state', 'year', 'quarter', 'transaction_name', 'payment_type'],\n",
    "    'agg_ins':      ['state', 'year', 'quarter', 'insurance_name', 'payment_type'],\n",
    "    'map_user':     ['state', 'year', 'quarter', 'hover_state'],\n",
    "    'map_trans':    ['state', 'year', 'quarter', 'location_name', 'metric_type'],\n",
    "    'map_insHover': ['state', 'year', 'quarter', 'location_name', 'metric_type'],\n",
    "    'map_insCntry': ['year', 'quarter', 'latitude', 'longitude'],\n",
    "    'top_user':     ['year', 'quarter', 'state_name', 'district_name', 'pincode'],\n",
    "    'top_trans':    ['year', 'quarter', 'state_entity', 'district_entity', 'pincode_entity'],\n",
    "    'top_ins':      ['year', 'quarter', 'state_entity', 'district_entity', 'pincode_entity'],\n",
    "}\n",
    "\n",
    "# Mapping from JSON type to actual MariaDB table name\n",
    "TABLE_NAME_MAP = {\n",
    "    'agg_user':     'agg_user',\n",
    "    'agg_trans':    'agg_trans',\n",
    "    'agg_ins':      'agg_ins',\n",
    "    'map_user':     'map_user',\n",
    "    'map_trans':    'map_trans',\n",
    "    'map_insHover': 'map_ins_hover',\n",
    "    'map_insCntry': 'map_ins_cntry',\n",
    "    'top_user':     'top_user',\n",
    "    'top_trans':    'top_trans',\n",
    "    'top_ins':      'top_ins',\n",
    "}\n",
    "\n",
    "# Check if a row already exists in the database (to avoid duplicates)\n",
    "def row_exists(db: DBConnect, type_name: str, record: dict) -> bool:\n",
    "    \"\"\"Return True if a row with the same natural key already exists.\"\"\"\n",
    "    table_name = TABLE_NAME_MAP.get(type_name, type_name)\n",
    "    if table_name not in db.metadata.tables:\n",
    "        db.refresh_metadata()\n",
    "    if table_name not in db.metadata.tables:\n",
    "        return False\n",
    "    table = db.metadata.tables[table_name]\n",
    "\n",
    "    key_cols = KEY_COLUMNS.get(type_name, [])\n",
    "    if not key_cols:\n",
    "        return False  # no key → treat as not existing\n",
    "\n",
    "    conds = []\n",
    "    for col in key_cols:\n",
    "        if col not in record:\n",
    "            # Key value missing in this row → can't match anything; treat as not existing\n",
    "            return False\n",
    "        conds.append(table.c[col] == record[col])\n",
    "\n",
    "    stmt = select(table.c[list(table.c.keys())[0]]).where(and_(*conds)).limit(1)\n",
    "\n",
    "    with db.engine.connect() as conn:\n",
    "        res = conn.execute(stmt).first()\n",
    "        return res is not None\n",
    "\n",
    "# Insert DataFrame rows into the database with duplicate skipping\n",
    "def insert_to_db(type_name: str, df: pd.DataFrame):\n",
    "    table_name = TABLE_NAME_MAP.get(type_name, type_name)\n",
    "    db = DBConnect()\n",
    "    for _, row in df.iterrows():\n",
    "        record = row.where(pd.notnull(row), None).to_dict()\n",
    "        record = normalize_year_quarter(record)\n",
    "        record = {k: to_db_scalar(v) for k, v in record.items()}\n",
    "\n",
    "        if row_exists(db, type_name, record):\n",
    "            continue  # skip duplicates by natural key\n",
    "        try:\n",
    "            db.insert(table_name, record)\n",
    "        except Exception as e:\n",
    "            print(f\"❌ DB Insert Error into {table_name}: {e}\")\n",
    "            print(f\"Row data -> {record}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aaef555d",
   "metadata": {},
   "source": [
    "### JSON Processing\n",
    "\n",
    "The `JsonProcess` function below handles all ten JSON structures used in the PhonePe Pulse data.\n",
    "It converts nested structures into flat Pandas DataFrames with consistent column names. If a JSON\n",
    "file type contains a list of entries (e.g. multiple transactions or multiple state metrics),\n",
    "the function returns multiple rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4fe51fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def JsonProcess(LoadedJson: dict, Type: str, state=None, year=None, quarter=None) -> pd.DataFrame:\n",
    "    d = LoadedJson.get('data', {}) or {}\n",
    "    success = LoadedJson.get('success')\n",
    "    code = LoadedJson.get('code')\n",
    "    response_ts = LoadedJson.get('responseTimestamp')\n",
    "\n",
    "    # Aggregated Users\n",
    "    if Type == 'agg_user':\n",
    "        return pd.DataFrame([{\n",
    "            'state': state, 'year': year, 'quarter': quarter,\n",
    "            'success': success, 'code': code,\n",
    "            'registered_users': (d.get('aggregated') or {}).get('registeredUsers'),\n",
    "            'app_opens': (d.get('aggregated') or {}).get('appOpens'),\n",
    "            'users_by_device': d.get('usersByDevice'),\n",
    "            'response_ts': response_ts\n",
    "        }])\n",
    "\n",
    "    # Aggregated Transactions\n",
    "    elif Type == 'agg_trans':\n",
    "        rows = []\n",
    "        for td in (d.get('transactionData') or []):\n",
    "            pi = (td.get('paymentInstruments') or [{}])[0]\n",
    "            rows.append({\n",
    "                'state': state, 'year': year, 'quarter': quarter,\n",
    "                'success': success, 'code': code,\n",
    "                'from_ts': d.get('from'), 'to_ts': d.get('to'),\n",
    "                'transaction_name': td.get('name'),\n",
    "                'payment_type': pi.get('type'),\n",
    "                'payment_count': pi.get('count'),\n",
    "                'payment_amount': pi.get('amount'),\n",
    "                'response_ts': response_ts\n",
    "            })\n",
    "        return pd.DataFrame(rows or [{}])\n",
    "\n",
    "    # Aggregated Insurance\n",
    "    elif Type == 'agg_ins':\n",
    "        rows = []\n",
    "        for td in (d.get('transactionData') or []):\n",
    "            pi = (td.get('paymentInstruments') or [{}])[0]\n",
    "            rows.append({\n",
    "                'state': state, 'year': year, 'quarter': quarter,\n",
    "                'success': success, 'code': code,\n",
    "                'from_ts': d.get('from'), 'to_ts': d.get('to'),\n",
    "                'insurance_name': td.get('name'),\n",
    "                'payment_type': pi.get('type'),\n",
    "                'payment_count': pi.get('count'),\n",
    "                'payment_amount': pi.get('amount'),\n",
    "                'response_ts': response_ts\n",
    "            })\n",
    "        return pd.DataFrame(rows or [{}])\n",
    "\n",
    "    # Map Users\n",
    "    elif Type == 'map_user':\n",
    "        rows = []\n",
    "        for st_name, info in (d.get('hoverData') or {}).items():\n",
    "            rows.append({\n",
    "                'state': state, 'year': year, 'quarter': quarter,\n",
    "                'success': success, 'code': code,\n",
    "                'hover_state': st_name,\n",
    "                'registered_users': (info or {}).get('registeredUsers'),\n",
    "                'app_opens': (info or {}).get('appOpens'),\n",
    "                'response_ts': response_ts\n",
    "            })\n",
    "        return pd.DataFrame(rows or [{}])\n",
    "\n",
    "    # Map Transactions\n",
    "    elif Type == 'map_trans':\n",
    "        rows = []\n",
    "        for item in (d.get('hoverDataList') or []):\n",
    "            for m in (item.get('metric') or []):\n",
    "                rows.append({\n",
    "                    'state': state, 'year': year, 'quarter': quarter,\n",
    "                    'success': success, 'code': code,\n",
    "                    'location_name': item.get('name'),\n",
    "                    'metric_type': m.get('type'),\n",
    "                    'metric_count': m.get('count'),\n",
    "                    'metric_amount': m.get('amount'),\n",
    "                    'response_ts': response_ts\n",
    "                })\n",
    "        return pd.DataFrame(rows or [{}])\n",
    "\n",
    "    # Map Insurance Country\n",
    "    elif Type == 'map_insCntry':\n",
    "        rows = []\n",
    "        meta = d.get('meta') or {}\n",
    "        grid = (d.get('data') or {})\n",
    "        cols = grid.get('columns') or []\n",
    "        data_rows = grid.get('data') or []\n",
    "        print(data_rows)\n",
    "        for r in data_rows:\n",
    "            row = dict(zip(cols, r))\n",
    "            rows.append({\n",
    "                'state': state, 'year': year, 'quarter': quarter,\n",
    "                'success': success, 'code': code,\n",
    "                'data_level': meta.get('dataLevel'),\n",
    "                'grid_level': meta.get('gridLevel'),\n",
    "                'percentiles': meta.get('percentiles'),\n",
    "                'latitude': row.get('lat'),\n",
    "                'longitude': row.get('lng'),\n",
    "                'metric_value': row.get('metric'),\n",
    "                'label': row.get('label'),\n",
    "                'response_ts': response_ts\n",
    "            })\n",
    "        return pd.DataFrame(rows or [{}])\n",
    "\n",
    "    # Map Insurance Hover\n",
    "    elif Type == 'map_insHover':\n",
    "        rows = []\n",
    "        for item in (d.get('hoverDataList') or []):\n",
    "            for m in (item.get('metric') or []):\n",
    "                rows.append({\n",
    "                    'state': state, 'year': year, 'quarter': quarter,\n",
    "                    'success': success, 'code': code,\n",
    "                    'location_name': item.get('name'),\n",
    "                    'metric_type': m.get('type'),\n",
    "                    'metric_count': m.get('count'),\n",
    "                    'metric_amount': m.get('amount'),\n",
    "                    'response_ts': response_ts\n",
    "                })\n",
    "        return pd.DataFrame(rows or [{}])\n",
    "\n",
    "    # Top Users\n",
    "    elif Type == 'top_user':\n",
    "        states_list = d.get('states') or []\n",
    "        districts = d.get('districts') or []\n",
    "        pincodes = d.get('pincodes') or []\n",
    "        max_len = max(len(states_list), len(districts), len(pincodes), 1)\n",
    "\n",
    "        def get_or_none(lst, idx, key):\n",
    "            return (lst[idx].get(key) if idx < len(lst) else None) if lst else None\n",
    "\n",
    "        rows = []\n",
    "        for i in range(max_len):\n",
    "            rows.append({\n",
    "                'state': state, 'year': year, 'quarter': quarter,\n",
    "                'success': success, 'code': code,\n",
    "                'state_name': get_or_none(states_list, i, 'name'),\n",
    "                'state_registered_users': get_or_none(states_list, i, 'registeredUsers'),\n",
    "                'district_name': get_or_none(districts, i, 'name'),\n",
    "                'district_registered_users': get_or_none(districts, i, 'registeredUsers'),\n",
    "                'pincode': get_or_none(pincodes, i, 'name'),\n",
    "                'pincode_registered_users': get_or_none(pincodes, i, 'registeredUsers'),\n",
    "                'response_ts': response_ts\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Top Transactions\n",
    "    elif Type == 'top_trans':\n",
    "        states_list = d.get('states') or []\n",
    "        districts = d.get('districts') or []\n",
    "        pincodes = d.get('pincodes') or []\n",
    "        max_len = max(len(states_list), len(districts), len(pincodes), 1)\n",
    "\n",
    "        def g(lst, idx, *path):\n",
    "            if idx >= len(lst):\n",
    "                return None\n",
    "            node = lst[idx]\n",
    "            for p in path:\n",
    "                node = node.get(p) if isinstance(node, dict) else None\n",
    "                if node is None:\n",
    "                    break\n",
    "            return node\n",
    "\n",
    "        rows = []\n",
    "        for i in range(max_len):\n",
    "            rows.append({\n",
    "                'state': state, 'year': year, 'quarter': quarter,\n",
    "                'success': success, 'code': code,\n",
    "                'state_entity': g(states_list, i, 'entityName'),\n",
    "                'state_metric_type': g(states_list, i, 'metric', 'type'),\n",
    "                'state_metric_count': g(states_list, i, 'metric', 'count'),\n",
    "                'state_metric_amount': g(states_list, i, 'metric', 'amount'),\n",
    "                'district_entity': g(districts, i, 'entityName'),\n",
    "                'district_metric_type': g(districts, i, 'metric', 'type'),\n",
    "                'district_metric_count': g(districts, i, 'metric', 'count'),\n",
    "                'district_metric_amount': g(districts, i, 'metric', 'amount'),\n",
    "                'pincode_entity': g(pincodes, i, 'entityName'),\n",
    "                'pincode_metric_type': g(pincodes, i, 'metric', 'type'),\n",
    "                'pincode_metric_count': g(pincodes, i, 'metric', 'count'),\n",
    "                'pincode_metric_amount': g(pincodes, i, 'metric', 'amount'),\n",
    "                'response_ts': response_ts\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    # Top Insurance\n",
    "    elif Type == 'top_ins':\n",
    "        states_list = d.get('states') or []\n",
    "        districts = d.get('districts') or []\n",
    "        pincodes = d.get('pincodes') or []\n",
    "        max_len = max(len(states_list), len(districts), len(pincodes), 1)\n",
    "\n",
    "        def g(lst, idx, *path):\n",
    "            if idx >= len(lst):\n",
    "                return None\n",
    "            node = lst[idx]\n",
    "            for p in path:\n",
    "                node = node.get(p) if isinstance(node, dict) else None\n",
    "                if node is None:\n",
    "                    break\n",
    "            return node\n",
    "\n",
    "        rows = []\n",
    "        for i in range(max_len):\n",
    "            rows.append({\n",
    "                'state': state, 'year': year, 'quarter': quarter,\n",
    "                'success': success, 'code': code,\n",
    "                'state_entity': g(states_list, i, 'entityName'),\n",
    "                'state_metric_type': g(states_list, i, 'metric', 'type'),\n",
    "                'state_metric_count': g(states_list, i, 'metric', 'count'),\n",
    "                'state_metric_amount': g(states_list, i, 'metric', 'amount'),\n",
    "                'district_entity': g(districts, i, 'entityName'),\n",
    "                'district_metric_type': g(districts, i, 'metric', 'type'),\n",
    "                'district_metric_count': g(districts, i, 'metric', 'count'),\n",
    "                'district_metric_amount': g(districts, i, 'metric', 'amount'),\n",
    "                'pincode_entity': g(pincodes, i, 'entityName'),\n",
    "                'pincode_metric_type': g(pincodes, i, 'metric', 'type'),\n",
    "                'pincode_metric_count': g(pincodes, i, 'metric', 'count'),\n",
    "                'pincode_metric_amount': g(pincodes, i, 'metric', 'amount'),\n",
    "                'response_ts': response_ts\n",
    "            })\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    else:\n",
    "        return pd.DataFrame([{'notice': f'Unknown Type: {Type}'}])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c68b98c",
   "metadata": {},
   "source": [
    "### Processing Folders\n",
    "\n",
    "The `ProcessFolder` function walks through the dataset directories, handling both year-level and state-level\n",
    "structures. It converts each JSON file into a DataFrame and inserts the rows into the corresponding MariaDB\n",
    "table, skipping duplicates automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea17437",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def ProcessFolder(PATH: str, Type: str):\n",
    "    for year in os.listdir(PATH):\n",
    "        year_path = os.path.join(PATH, year)\n",
    "        if not os.path.isdir(year_path):\n",
    "            continue\n",
    "        # Year-level directory\n",
    "        if year.lower() != 'state':\n",
    "            for file in os.listdir(year_path):\n",
    "                if not file.endswith('.json'):\n",
    "                    continue\n",
    "                quarter = os.path.splitext(file)[0]\n",
    "                file_path = os.path.join(year_path, file)\n",
    "                data = load_json(file_path)\n",
    "                df = JsonProcess(data, Type, year=year, quarter=quarter)\n",
    "                insert_to_db(Type, df)\n",
    "        else:\n",
    "            # State-level directory\n",
    "            for state in os.listdir(year_path):\n",
    "                state_path = os.path.join(year_path, state)\n",
    "                if not os.path.isdir(state_path):\n",
    "                    continue\n",
    "                print(f'Processing state: {state}')\n",
    "                for styear in os.listdir(state_path):\n",
    "                    year_folder = os.path.join(state_path, styear)\n",
    "                    if not os.path.isdir(year_folder):\n",
    "                        continue\n",
    "                    for file in os.listdir(year_folder):\n",
    "                        if not file.endswith('.json'):\n",
    "                            continue\n",
    "                        quarter = os.path.splitext(file)[0]\n",
    "                        file_path = os.path.join(year_folder, file)\n",
    "                        data = load_json(file_path)\n",
    "                        df = JsonProcess(data, Type, state=state.lower(), year=styear, quarter=quarter)\n",
    "                        insert_to_db(Type, df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7a5dd9a",
   "metadata": {},
   "source": [
    "## Executing the Loader\n",
    "\n",
    "Uncomment any of the calls below to process the corresponding dataset. Each call reads all\n",
    "available JSON files under the specified path and loads them into MariaDB, skipping any duplicate rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4655406",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Example usage:\n",
    "# ProcessFolder(agg_user_PATH,  'agg_user')\n",
    "# ProcessFolder(agg_trans_PATH, 'agg_trans')\n",
    "# ProcessFolder(agg_ins_PATH,   'agg_ins')\n",
    "\n",
    "# ProcessFolder(map_user_PATH,     'map_user')\n",
    "# ProcessFolder(map_trans_PATH,    'map_trans')\n",
    "# ProcessFolder(map_insHover_PATH, 'map_insHover')\n",
    "# ProcessFolder(map_insCntry_PATH, 'map_insCntry')\n",
    "\n",
    "# ProcessFolder(top_user_PATH,  'top_user')\n",
    "# ProcessFolder(top_trans_PATH, 'top_trans')\n",
    "# ProcessFolder(top_ins_PATH,   'top_ins')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GuviProj",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
